{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"_연습문제풀이3_딥러닝감성분석.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPgkU3R92tUsWv6vu4htHse"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ShZjJaOd2ynp","colab_type":"text"},"source":["# 네이버 영화평 자료로 딥러닝 감성분석을 수행하시오"]},{"cell_type":"code","metadata":{"id":"B5UoQD-z2vOj","colab_type":"code","colab":{}},"source":["# from google.colab import auth\n","# auth.authenticate_user()\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"K5Xpnzgw2v0X","colab_type":"code","colab":{}},"source":["# 형태소분석기 관련 설치\n","!apt-get update\n","!apt-get install g++ openjdk-8-jdk\n","\n","!pip install JPype1==0.7.0\n","!pip install rhinoMorph"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VQ3ZREME5r0t","colab_type":"code","colab":{}},"source":["# 경로 변경\n","cd /content/gdrive/My Drive/pytest/"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y1MDKtqW5t7x","colab_type":"code","colab":{}},"source":["# 데이터 로딩\n","\n","def read_data(filename, encoding='cp949'):                # 읽기 함수 정의\n","  with open(filename, 'r', encoding=encoding) as f:\n","    data = [line.split('\\t') for line in f.read().splitlines()]\n","    data = data[1:]                 # txt 파일의 헤더(id document label)는 제외하기\n","  return data\n","\n","def write_data(data, filename, encoding='cp949'):         # 쓰기 함수 정의\n","  with open(filename, 'w', encoding=encoding) as f:\n","    f.write(data)\n","\n","data = read_data('ratings_small.txt', encoding='cp949')  # 전체파일은 ratings.txt (긍정 1만, 부정 1만)\n","\n","print(len(data))\n","print(len(data[0])) \n","print(data[0])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bPgQJjHY5_2P","colab_type":"code","colab":{}},"source":["# 전체 데이터 형태소 분석\n","import rhinoMorph\n","rn = rhinoMorph.startRhino()\n","\n","morphed_data = ''\n","for data_each in data:\n","  morphed_data_each = rhinoMorph.onlyMorph_list(rn, data_each[1], pos=['NNG', 'NNP', 'VV', 'VA', 'XR', 'IC', 'MM', 'MAG', 'MAJ'], eomi=True)\n","  joined_data_each = ' '.join(morphed_data_each)\t\t\t        # 문자열을 하나로 연결\n","  if joined_data_each:                                      \t# 내용이 있는 경우만 저장하게 함\n","    morphed_data += data_each[0]+\"\\t\"+joined_data_each+\"\\t\"+data_each[2]+\"\\n\"\n","    \n","# 형태소 분석된 파일 저장\n","write_data(morphed_data, 'ratings_morphed.txt', encoding='cp949')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zDqFYVqo6BdF","colab_type":"code","colab":{}},"source":["print(morphed_data)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"w2B0Inu66F-X","colab_type":"code","colab":{}},"source":["# 기분석된 데이터 로딩\n","\n","data = read_data('ratings_morphed.txt', encoding='cp949')\n","print(len(data))                              # 일부는 내용이 남지 않아 제외 됨\n","print(len(data[0]))                           # 3개의 컬럼(id, 본문, 긍부정)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mOYvZjioDYmt","colab_type":"code","colab":{}},"source":["print(data)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GP6sOMVv8V0T","colab_type":"code","colab":{}},"source":["# 훈련데이터와 테스트데이터 분리 (수동)\n","import random\n","random.shuffle(data)                            \t\t# 랜덤하게 섞는다\n","\n","data_70 = int(len(data)*0.7)\t\t\t\t\t              # 전체 데이터 크기의 70% 숫자를 찾는다\n","train_data = data[:data_70] \t\t\t\t\t              # 앞에서 70% 부분을 잘라 훈련데이터로\n","test_data = data[data_70:] \t\t\t\t\t                # 그 뒷부분을 테스트데이터로\n","\n","print('train data length:', len(train_data))    \t\t# 344\n","print('test data length:', len(test_data))      \t\t# 148\n","\n","# 훈련데이터 요소 분리\n","train_texts = [line[1] for line in train_data]      \t# 훈련데이터 본문\n","train_labels = [int(line[2]) for line in train_data]  # 훈련데이터 긍부정 부분. int로 변환한다\n","\n","# 테스트데이터 요소 분리\n","test_texts = [line[1] for line in test_data]        \t# 테스트데이터 본문\n","test_labels = [int(line[2]) for line in test_data]    # 테스트데이터 긍부정 부분. int로 변환한다"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BmsOoKizE5Ih","colab_type":"code","colab":{}},"source":["# Data 확인\n","print('texts 0:', train_texts[0])\n","print('texts len:', len(train_texts))\n","\n","print('labels 0:', train_labels[0])\n","print('labels len:', len(train_labels))\n","\n","print('train_labels type:', type(train_labels))\n","print('train_labels type:', type(train_labels[0]))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VfK-vSu0HQST","colab_type":"code","colab":{}},"source":["# Data Tokenizing\n","# 텍스트에 사용된 단어의 종류를 빈도 순으로 정렬하는 작업을 수행한다\n","%tensorflow_version 2.x\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import numpy as np\n","import math\n","\n","validation_ratio = math.floor(len(train_texts) * 0.3)  \t# 검증 샘플은 전체의 30%로 한다\n","max_words = 10000               \t\t\t                  # 데이터셋에서 가장 빈도 높은 10,000 개의 단어만 사용한다\n","maxlen = 200\t\t\t\t\t                                  # 항상 200 단어가 되도록 길이를 고정한다\n","\n","tokenizer = Tokenizer(num_words=max_words)\t            # 상위빈도 10,000 개의 단어만을 추려내는 Tokenizer 객체 생성\n","tokenizer.fit_on_texts(train_texts)     \t\t\t          # 단어 인덱스를 구축한다 \n","word_index = tokenizer.word_index           \t\t        # 단어 인덱스만 가져온다 "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-j-eEcdWHxOS","colab_type":"code","colab":{}},"source":["# Tokenizing 결과 확인\n","print('전체에서 %s개의 고유한 토큰을 찾았습니다.' % len(word_index))\n","print('word_index type: ', type(word_index))\n","print('word_index: ', word_index)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pRWQPimuHywn","colab_type":"code","colab":{}},"source":["# Data Sequencing\n","# 문자를 숫자로 변환하는 작업을 수행한다\n","# 상위 빈도 10,000(max_words)개의 단어만 추출하여 word_index의 숫자 리스트로 변환한다.\n","data = tokenizer.texts_to_sequences(train_texts)\t\t# Tokenizer 결과가 여기서 반영된다.\n","\n","print('data 0:', data[0])\n","print('texts 0:', train_texts[0])\t\t\t\t           # texts[0]의 본래 단어들"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"b6H51fajH8xi","colab_type":"code","colab":{}},"source":["print(type(train_texts))\n","print(type(data))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KzyLLIpHIFg2","colab_type":"code","colab":{}},"source":["# Data Pading\n","data = pad_sequences(data, maxlen=maxlen) \n","\n","print('data:', data)\n","print('data 0:', data[0])\n","print(len(data[0]))\n","\n","print(word_index)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xpalxDNbILdw","colab_type":"code","colab":{}},"source":["print(type(train_texts))\n","print(type(data))\n","print(data.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"M6KxP2OqIQXP","colab_type":"code","colab":{}},"source":["# One-Hot-Encoding\n","def to_one_hot(sequences, dimension):\n","  results = np.zeros((len(sequences), dimension))\n","  for i, sequence in enumerate(sequences):\n","    results[i, sequence] = 1.\n","  return results\n","\n","\n","data = to_one_hot(data, dimension=max_words) \n","labels = np.asarray(train_labels).astype('float32')   "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nXw6lI9OIw1Z","colab_type":"code","colab":{}},"source":["# One-Hot-Encoding 결과 확인\n","\n","print('data:', data)\n","len(data[0])\t\t\t\t\t# dimension=10000으로 했으므로 각 행은 10,000개를 가지고 있다\n","print('data [0][0:100]:', data[0][0:100])\n","\n","print(word_index)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"v2dnos7XIyx0","colab_type":"code","colab":{}},"source":["print(type(train_texts))\n","print(type(data))\n","print(data.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4GXcex83I35S","colab_type":"code","colab":{}},"source":["# Train 데이터와 Validation 데이터 준비\n","\n","print('데이터 텐서의 크기:', data.shape)  \t\t      # (344, 10000)\n","print('레이블 텐서의 크기:', labels.shape) \t\t      # (344,) data와 label이 모두 2D 텐서가 되었음\n","\n","indices = np.arange(data.shape[0]) \t\t              # 0 ~ 344 까지의 숫자를 생성\n","np.random.shuffle(indices)     \t\t\t                # 0 ~ 344 까지의 숫자를 랜덤하게 섞음\n","data = data[indices]    \t\t\t\t                    # 이것을 인덱스로 하여 2D 텐서 데이터를 섞음 \n","labels = labels[indices]\t\t\t\t                    # label도 같은 순서로 섞음 \n","\n","print(indices)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LDFNKZ5_QcHx","colab_type":"code","colab":{}},"source":["# 훈련데이터와 검증데이터 분리\n","x_train = data[validation_ratio:] \t\t\t      # 훈련데이터의 70%를 훈련데이터\n","y_train = labels[validation_ratio:] \t\t\t    # 훈련데이터의 70%를 훈련데이터 Label (data와 labels는 같은 순서)\n","x_val = data[:validation_ratio] \t\t\t        # 훈련데이터의 30%를 검증데이터\n","y_val = labels[:validation_ratio] \t\t\t      # 훈련데이터의 30%를 검증데이터 Label"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aJ2AYfXPQgGo","colab_type":"code","colab":{}},"source":["# 모델 정의하기\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","\n","model = Sequential()                                              # 모델을 새로 정의\n","\n","model.add(Dense(64, activation='relu', input_shape=(max_words,)))\t# 첫 번째 은닉층\n","model.add(Dense(32, activation='relu'))                           # 두 번째 은닉층\n","model.add(Dense(1, activation='sigmoid'))                 \t\t    # 출력층"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RnFV-dstQkOY","colab_type":"code","colab":{}},"source":["# 모델 요약 출력\n","model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wo6myYK_Qlj2","colab_type":"code","colab":{}},"source":["# Compile & Train Model\n","# 모델 컴파일\n","# 가중치 업데이트 방법은 RMSprop을 사용하였다. 이동평균의 방법을 도입하여 조절해간다\n","# 신경망의 출력이 확률이므로 crossentropy를 사용하는 것이 최선이다\n","# crossentropy는 원본의 확률 분포와 예측의 확률 분포를 측정하여 조절해 간다\n","# 또한 이진 분류이므로 binary_crossentropy를 사용한다\n","model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n","\n","# 모델 훈련\n","# 32개씩 미니 배치를 만들어 10번의 epoch로 훈련한다. 보통 32개에서 시작하여 512개까지 중에서 찾는다\n","# 훈련 데이터로 훈련하고, 검증 데이터로 검증한다 \n","# 반환값의 history는 훈련하는 동안 발생한 모든 정보를 담고 있는 딕셔너리이다\n","history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))\n","history_dict = history.history"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_A1CRFNCQr_p","colab_type":"code","colab":{}},"source":["# 경로 변경\n","cd /content/gdrive/My Drive/pytest/"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cdagiUPAQuKM","colab_type":"code","colab":{}},"source":["# Save Model\n","# multidimensional numpy arrays를 저장할 수 있는 h5 file(HDF) 포맷으로 저장한다\n","model.save('text_binary_model.h5')\n","\n","\n","# 훈련데이터에서 사용된 상위빈도 10,000개의 단어로 된 Tokenizer 저장\n","# 새로 입력되는 문장에서도 같은 단어가 추출되게 한다\n","import pickle\n","with open('text_binary_tokenizer.pickle', 'wb') as handle:\n","  pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wQMaEm9VQzhe","colab_type":"code","colab":{}},"source":["# Accuracy & Loss 확인\n","# history 딕셔너리 안에 있는 정확도와 손실값을 가져와 본다\n","\n","acc = history.history['acc']\n","val_acc = history.history['val_acc']\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","print('Accuracy of each epoch:', acc)\t\t# [0.79, 0.90, 0.93, 0.94, 0.96, 0.97, 0.98, 0.98, 0.98, 0.99]\n","epochs = range(1, len(acc) +1)\t\t\t# range(1, 11)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QQ9sbghfQ2y2","colab_type":"code","colab":{}},"source":["# Plotting Accuracy\n","import matplotlib.pyplot as plt\n","\n","# 훈련데이터의 정확도에 비해 검증데이터의 정확도는 낮게 나타난다\n","# epoch가 높아지면 모델은 훈련데이터에 매우 민감해져 오히려 새로운 데이터를 잘 못 맞춘다\n","plt.plot(epochs, acc, 'bo', label='Training Acc')\n","plt.plot(epochs, val_acc, 'b', label='Validation Acc')\n","plt.title('Training and validation accuracy')\n","plt.legend()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WYsgvVpJQ42B","colab_type":"code","colab":{}},"source":["# Plotting Loss\n","plt.figure()    # 새로운 그림을 그린다\n","\n","# 훈련데이터의 손실값은 낮아지나, 검증데이터의 손실값은 높아진다\n","# 손실값은 오류값을 말한다. 예측과 정답의 차이를 거리 계산으로 구한 값이다\n","plt.plot(epochs, loss, 'bo', label='Training Loss')\n","plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n","plt.title('Training and validation loss')\n","plt.legend()\n","\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"i_VaZCIbQ9zJ","colab_type":"code","colab":{}},"source":["# Load Model\n","import os\n","from tensorflow.keras.models import load_model\n","\n","filepath = '/content/gdrive/My Drive/pytest/'\n","os.chdir(filepath)\n","print(\"Current Directory:\", os.getcwd())\n","\n","loaded_model = load_model('text_binary_model.h5')\n","print(\"model loaded:\", loaded_model)\n","\n","with open('text_binary_tokenizer.pickle', 'rb') as handle:\n","  loaded_tokenizer = pickle.load(handle)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Le4dCvXiRBtV","colab_type":"code","colab":{}},"source":["# Data 확인\n","print('texts:', test_texts[0])\n","print('texts len:', len(test_texts))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fxfceA8HRPme","colab_type":"code","colab":{}},"source":["# Data Sequencing\n","# 문자열을 word_index의 숫자 리스트로 변환\n","data = loaded_tokenizer.texts_to_sequences(test_texts)\n","\n","# padding으로 문자열의 길이를 고정시킨다\n","data = pad_sequences(data, maxlen=maxlen) \n","\n","# test 데이터를 원-핫 인코딩한다\n","x_test = to_one_hot(data, dimension=max_words)\n","\n","# label을 list에서 넘파이 배열로 변환. 결과가 0 또는 1만 나오므로 이와같이 int32로 저장해도 된다.\n","y_test = np.asarray(test_labels)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EIn-Nm7xS2_G","colab_type":"code","colab":{}},"source":["# Test Data Evaluation\n","test_eval = loaded_model.evaluate(x_test, y_test)\t  # 모델에 분류할 데이터와 그 정답을 같이 넣어준다\n","print('prediction model loss & acc:', test_eval)\t\t# 모델이 분류한 결과와 입력된 정답을 비교한 결과"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qWrsapFnS8vc","colab_type":"code","colab":{}},"source":["# 1개 데이터 예측\n","text = \"평점이 왜 낮은지 모르겠다. 긴장감과 스릴감이 진짜 최고다.\"    # 데이터를 String 타입으로 만든다\n","\n","import rhinoMorph\n","rn = rhinoMorph.startRhino()\n","\n","# 똑같은 품사와 똑같은 eomi 옵션을 사용하여 형태소분석한다\n","morphed_text = rhinoMorph.onlyMorph_list(rn, text, pos=['NNG', 'NNP', 'VV', 'VA', 'XR', 'IC', 'MM', 'MAG', 'MAJ'], eomi=True)   \n","joined_morphed_text = [' '.join(morphed_text)]          # 결과를 리스트로 만들어야 한다\n","\n","print('morphed_text:', morphed_text)\t\t\t\t            # 형태소 분석 결과\n","print('joined morphed_text:', joined_morphed_text)\t    # 문자열을 공백으로 연결한다\n","\n","data = loaded_tokenizer.texts_to_sequences(joined_morphed_text)\n","data = pad_sequences(data, maxlen=maxlen)\n","x_test = to_one_hot(data, dimension=max_words)\n","\n","prediction = loaded_model.predict(x_test)\n","print(\"Result:\", prediction)\t\t\t\t\t                  # [[0.7647794]]. 1(긍정)이 될 확률이 76.4%"],"execution_count":0,"outputs":[]}]}